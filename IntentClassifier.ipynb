{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IntentClassifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNw28tn6m2JyrtwPzM0WqNZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronitd/IntentClassifier/blob/main/IntentClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tWOBWl6KhVX"
      },
      "source": [
        ""
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY5j9g3YMlG9"
      },
      "source": [
        "Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcFZBtKuMpRk"
      },
      "source": [
        "import json\r\n",
        "import torch\r\n",
        "from torchtext import data\r\n",
        "import numpy as np\r\n",
        "import pandas as pd"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kirRjL_vNIV3"
      },
      "source": [
        "Selecting the random 20 labels and creating csv files for train, validation and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQGSn7mERelT",
        "outputId": "be2388b8-b86f-4434-e5bf-385a49119f43"
      },
      "source": [
        "def save_csv(fileName, content):\r\n",
        "  my_df = pd.DataFrame(content)\r\n",
        "  my_df.to_csv(fileName+'.csv', index=False, header=[\"Text\", \"Label\"])\r\n",
        "\r\n",
        "\r\n",
        "np.random.seed(10)\r\n",
        "with open( 'data_full.json', 'r' ) as f:\r\n",
        "  org_data = json.load( f )\r\n",
        "data_1 = np.array(org_data[\"train\"])\r\n",
        "print(np.unique(data_1[:, 1]).shape )\r\n",
        "labels = np.unique(data_1[:, 1])\r\n",
        "labels = np.random.choice(labels,20, replace=False)\r\n",
        "print(labels)\r\n",
        "print(\"Labels shape\", labels.shape)\r\n",
        "\r\n",
        "\r\n",
        "index = np.isin(data_1[:, 1], labels, assume_unique=True)\r\n",
        "save_csv(\"train\", data_1[index])\r\n",
        "\r\n",
        "val_data = np.array(org_data[\"val\"])\r\n",
        "index = np.isin(val_data[:, 1], labels, assume_unique=True)\r\n",
        "print(\"Val\", val_data[index].shape)\r\n",
        "save_csv(\"val\", val_data[index])\r\n",
        "\r\n",
        "test_data = np.array(org_data[\"test\"])\r\n",
        "index = np.isin(test_data[:, 1], labels, assume_unique=True)\r\n",
        "print(\"Test\", test_data[index].shape)\r\n",
        "save_csv(\"test\", test_data[index])\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150,)\n",
            "['plug_type' 'shopping_list' 'book_hotel' 'pto_used' 'goodbye'\n",
            " 'international_fees' 'mpg' 'meal_suggestion' 'exchange_rate'\n",
            " 'ingredient_substitution' 'maybe' 'what_can_i_ask_you'\n",
            " 'improve_credit_score' 'account_blocked' 'carry_on'\n",
            " 'shopping_list_update' 'pin_change' 'do_you_have_pets' 'change_ai_name'\n",
            " 'direct_deposit']\n",
            "Labels shape (20,)\n",
            "Val (400, 2)\n",
            "Test (600, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RImt4B-nWvw"
      },
      "source": [
        "**Pre-Processing** Converting sentences to word using spacy, converting the data into lower case(I checked the few sentesnces all of them were lower case, this steps is for just to be sure.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANGz7nL0nVYC",
        "outputId": "aa7767d8-0ccb-42eb-a220-fa184624f820"
      },
      "source": [
        "TEXT = data.Field(tokenize='spacy',lower=True, batch_first=True,include_lengths=True)\r\n",
        "LABEL = data.LabelField(dtype = torch.float,batch_first=True)\r\n",
        "fields = [('text', TEXT), ('label', LABEL)]  \r\n",
        "\r\n",
        "train, val, test = data.TabularDataset.splits(\r\n",
        "        path='./', train='train.csv',\r\n",
        "        validation='val.csv', test='test.csv', format='csv',\r\n",
        "        fields=fields, skip_header = True)\r\n",
        "# #print preprocessed text\r\n",
        "print(vars(train.examples[0]))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['what', 'steps', 'should', 'i', 'take', 'to', 'improve', 'my', 'credit', 'score'], 'label': 'improve_credit_score'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHZ2Gsn4Nno_"
      },
      "source": [
        "Building the vocabulary that is converting the tokens into vector representation which would be feature representation of the word. Using Glove word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAmE_9xcOSQ1",
        "outputId": "c98bf9dc-8db6-4a7e-f452-3e3466b81607"
      },
      "source": [
        "#initialize glove embeddings\r\n",
        "TEXT.build_vocab(train,min_freq=3,vectors = \"glove.6B.100d\")  \r\n",
        "LABEL.build_vocab(train)\r\n",
        "\r\n",
        "#No. of unique tokens in text\r\n",
        "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\r\n",
        "\r\n",
        "#No. of unique tokens in label\r\n",
        "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\r\n",
        "\r\n",
        "#Commonly used words\r\n",
        "print(TEXT.vocab.freqs.most_common(10))  \r\n",
        "\r\n",
        "#Word dictionary\r\n",
        "print(TEXT.vocab.stoi)\r\n",
        "\r\n",
        "print(\"Labels\", LABEL.vocab.freqs)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of TEXT vocabulary: 580\n",
            "Size of LABEL vocabulary: 20\n",
            "[('i', 947), ('my', 685), ('to', 675), ('what', 469), ('you', 451), ('the', 410), ('for', 404), ('a', 359), ('can', 330), ('on', 330)]\n",
            "defaultdict(<function _default_unk_index at 0x7faf65e1aea0>, {'<unk>': 0, '<pad>': 1, 'i': 2, 'my': 3, 'to': 4, 'what': 5, 'you': 6, 'the': 7, 'for': 8, 'a': 9, 'can': 10, 'on': 11, 'in': 12, 'me': 13, 'do': 14, 'how': 15, 'of': 16, 'is': 17, 'account': 18, 'have': 19, 'list': 20, 'shopping': 21, 'are': 22, ',': 23, 'need': 24, 'from': 25, 'if': 26, 'it': 27, 'know': 28, 'many': 29, 'with': 30, 'credit': 31, 'use': 32, 'carry': 33, 'pin': 34, 'tell': 35, 'change': 36, 'this': 37, 'up': 38, \"'s\": 39, 'direct': 40, 'be': 41, 'get': 42, 'card': 43, 'deposit': 44, 'does': 45, 'please': 46, 'there': 47, 'bank': 48, 'score': 49, 'and': 50, 'your': 51, 'car': 52, 'name': 53, 'why': 54, 'dollars': 55, 'number': 56, 'would': 57, 'days': 58, 'like': 59, 'set': 60, 'hotel': 61, 'will': 62, 'fees': 63, 'pets': 64, \"'m\": 65, 'that': 66, 'am': 67, 'add': 68, 'any': 69, 'plug': 70, 'want': 71, 'mpg': 72, 'used': 73, '-': 74, 'about': 75, 'milk': 76, 'off': 77, 'converter': 78, 'help': 79, 'much': 80, 'out': 81, 'transaction': 82, 'good': 83, 'kind': 84, 'exchange': 85, 'meal': 86, 'or': 87, \"n't\": 88, 'not': 89, 'dinner': 90, 'take': 91, 'when': 92, 'put': 93, 'find': 94, 'new': 95, 'ons': 96, 'so': 97, 'make': 98, 'suggest': 99, 'airlines': 100, 'room': 101, 'substitute': 102, 'delta': 103, 'hold': 104, 'one': 105, 'rate': 106, 'things': 107, \"'d\": 108, 'ask': 109, 'city': 110, 'go': 111, 'instead': 112, 'now': 113, 'taken': 114, 'vacation': 115, 'yen': 116, 'give': 117, 'paycheck': 118, 'should': 119, 'checking': 120, 'going': 121, 'type': 122, 'could': 123, 'international': 124, 'was': 125, '1234': 126, 'book': 127, 'using': 128, 'ai': 129, 'far': 130, 'pto': 131, 'restrictions': 132, 'see': 133, 'socket': 134, 'flight': 135, 'near': 136, 'people': 137, 'sure': 138, \"'ve\": 139, 'let': 140, 'answer': 141, 'suggestion': 142, 'types': 143, 'all': 144, 'at': 145, 'between': 146, 'later': 147, 'an': 148, 'charged': 149, 'cream': 150, 'either': 151, 'frozen': 152, 'through': 153, '!': 154, 'call': 155, 'chase': 156, 'gas': 157, 'kinds': 158, 'questions': 159, 'subjects': 160, 'which': 161, 'did': 162, 'gallon': 163, 'highway': 164, 'may': 165, 'mexico': 166, 'no': 167, 'some': 168, 'sugar': 169, 'talk': 170, 'ways': 171, 'amex': 172, 'as': 173, 'bread': 174, 'charge': 175, 'improve': 176, 'march': 177, 'miles': 178, 'own': 179, 'per': 180, 'pesos': 181, 'place': 182, 'reviews': 183, 'soda': 184, 'us': 185, 'america': 186, 'ca': 187, 'cat': 188, 'fee': 189, 'fuel': 190, 'maybe': 191, 'monday': 192, 'nice': 193, 'plugs': 194, 'reason': 195, 'rubles': 196, 'salt': 197, 'swap': 198, 'think': 199, 'time': 200, '10': 201, 'dog': 202, 'euros': 203, 'has': 204, 'hurt': 205, 'italy': 206, 'keep': 207, 'mileage': 208, 'pet': 209, 'remove': 210, 'savings': 211, 'they': 212, 'american': 213, 'april': 214, 'blocked': 215, 'build': 216, 'carrots': 217, 'dogs': 218, 'pounds': 219, 'say': 220, 'show': 221, 'stay': 222, 'baking': 223, 'cats': 224, 'check': 225, 'goodbye': 226, 'japan': 227, 's': 228, 'setting': 229, 'able': 230, 'amount': 231, 'bye': 232, 'conversion': 233, 'extra': 234, 'flights': 235, 'got': 236, 'italian': 237, 'look': 238, 'policy': 239, 'really': 240, 'them': 241, 'total': 242, 'united': 243, 'already': 244, 'associated': 245, 'been': 246, 'deposited': 247, 'food': 248, 'great': 249, 'pay': 250, 'person': 251, 'possible': 252, 'rules': 253, 'transactions': 254, 'until': 255, 'usd': 256, '1st': 257, '2': 258, 'allowed': 259, 'bananas': 260, 'butter': 261, 'different': 262, 'directly': 263, 'dollar': 264, 'economy': 265, 'england': 266, 'familiar': 267, 'fargo': 268, 'locked': 269, 'point': 270, 'sour': 271, 'steps': 272, 'sub': 273, 'visit': 274, 'wanna': 275, 'water': 276, 'we': 277, 'well': 278, 'wells': 279, 'work': 280, 'year': 281, 'york': 282, '4': 283, 'block': 284, 'calling': 285, 'france': 286, 'friday': 287, 'had': 288, 'idea': 289, 'mexican': 290, 'recipe': 291, 'remember': 292, 'respond': 293, 'something': 294, 'sort': 295, 'southwest': 296, 'suggestions': 297, 'talking': 298, '2nd': 299, '3rd': 300, '5': 301, '8': 302, 'acceptable': 303, 'again': 304, 'buy': 305, 'canada': 306, 'cars': 307, 'denver': 308, 'down': 309, 'flour': 310, 'here': 311, 'hotels': 312, 'more': 313, 'pepper': 314, 'placed': 315, 'read': 316, 'right': 317, 'sorts': 318, 'spain': 319, 'start': 320, 'switch': 321, 'visa': 322, 'way': 323, 'while': 324, 'yogurt': 325, '11': 326, '15': 327, '4th': 328, '5th': 329, '6': 330, 'animals': 331, 'australia': 332, 'cause': 333, 'chicago': 334, 'clue': 335, 'country': 336, 'currency': 337, 'current': 338, 'delete': 339, 'first': 340, 'ideas': 341, 'into': 342, 'items': 343, 'juice': 344, 'manhattan': 345, 'must': 346, 'needed': 347, 'oil': 348, 'okay': 349, 'other': 350, 'outlets': 351, 'pittsburgh': 352, 'russia': 353, 'seattle': 354, 'update': 355, 'wednesday': 356, 'wine': 357, \"'ll\": 358, '200': 359, '500': 360, 'access': 361, 'added': 362, 'allow': 363, 'anything': 364, 'apple': 365, 'around': 366, 'barcelona': 367, 'best': 368, 'better': 369, 'big': 370, 'booked': 371, 'but': 372, 'by': 373, 'cali': 374, 'chatting': 375, 'cherrios': 376, 'columbus': 377, 'cuisine': 378, 'dallas': 379, 'decrease': 380, 'driving': 381, 'eggs': 382, 'flying': 383, 'forgot': 384, 'freeze': 385, 'fries': 386, 'germany': 387, 'lakewood': 388, 'london': 389, 'lower': 390, 'might': 391, 'nt': 392, 'opening': 393, 'receive': 394, 'required': 395, 'rice': 396, 'spirit': 397, 'ta': 398, 'tampa': 399, 'thanks': 400, 'tomato': 401, 'tuesday': 402, 'who': 403, 'yens': 404, '01': 405, '2020': 406, '3': 407, '50': 408, 'ahead': 409, 'almond': 410, 'animal': 411, 'assist': 412, 'austin': 413, 'available': 414, 'bag': 415, 'being': 416, 'betty': 417, 'blue': 418, 'boa': 419, 'both': 420, 'bring': 421, 'brown': 422, 'canadian': 423, 'capital': 424, 'china': 425, 'confused': 426, 'count': 427, 'currently': 428, 'day': 429, 'discover': 430, 'dropping': 431, 'enough': 432, 'euro': 433, 'evans': 434, 'federal': 435, 'five': 436, 'fund': 437, 'india': 438, 'information': 439, 'jet': 440, 'limit': 441, 'meals': 442, 'money': 443, 'november': 444, 'ny': 445, 'ok': 446, 'olive': 447, 'oranges': 448, 'outlet': 449, 'owner': 450, 'paid': 451, 'paychecks': 452, 'peanut': 453, 'rename': 454, 'retirement': 455, 'scotland': 456, 'sick': 457, 'south': 458, 'special': 459, 'thai': 460, 'times': 461, 'today': 462, 'traveling': 463, 'true': 464, 'vegas': 465, 'vinegar': 466, 'whether': 467, 'x': 468, 'z': 469, \"'re\": 470, '20': 471, '3/10': 472, '8th': 473, '9th': 474, ';': 475, 'adios': 476, 'apples': 477, 'bags': 478, 'because': 479, 'blocking': 480, 'bob': 481, 'british': 482, 'building': 483, 'called': 484, 'changing': 485, 'chat': 486, 'cheese': 487, 'choose': 488, 'college': 489, 'comes': 490, 'conversation': 491, 'converters': 492, 'cook': 493, 'corn': 494, 'cumin': 495, 'decide': 496, 'depleted': 497, 'deposits': 498, 'dish': 499, 'dishes': 500, 'else': 501, 'everything': 502, 'exactly': 503, 'explain': 504, 'figure': 505, 'fly': 506, 'french': 507, 'garlic': 508, 'getting': 509, 'ginger': 510, 'glad': 511, 'gone': 512, 'half': 513, 'hawaiian': 514, 'hear': 515, 'home': 516, 'honey': 517, 'ice': 518, 'iceland': 519, 'include': 520, 'ireland': 521, 'january': 522, 'korea': 523, 'leave': 524, 'limits': 525, 'los': 526, 'luggage': 527, 'market': 528, 'mastercard': 529, 'navy': 530, 'ohio': 531, 'orlando': 532, 'paper': 533, 'paris': 534, 'personal': 535, 'peso': 536, 'pick': 537, 'pizza': 538, 'prefer': 539, 'protect': 540, 'pull': 541, 'raise': 542, 'rated': 543, 'recommend': 544, 'refer': 545, 'replace': 546, 'reset': 547, 'reviewed': 548, 'rome': 549, 'salem': 550, 'san': 551, 'seeing': 552, 'singapore': 553, 'soon': 554, 'sorry': 555, 'specific': 556, 'sterling': 557, 'surcharge': 558, 'thailand': 559, 'tips': 560, 'tonight': 561, 'trade': 562, 'travel': 563, 'tumeric': 564, 'uk': 565, 'unable': 566, 'understand': 567, 'unsure': 568, 'usaa': 569, 'usage': 570, 'vehicle': 571, 'versed': 572, 'vietnamese': 573, 'virgin': 574, 'wanting': 575, 'white': 576, 'ya': 577, 'yes': 578, 'yourself': 579})\n",
            "Labels Counter({'improve_credit_score': 100, 'shopping_list_update': 100, 'what_can_i_ask_you': 100, 'maybe': 100, 'mpg': 100, 'carry_on': 100, 'pto_used': 100, 'ingredient_substitution': 100, 'do_you_have_pets': 100, 'plug_type': 100, 'exchange_rate': 100, 'pin_change': 100, 'goodbye': 100, 'account_blocked': 100, 'international_fees': 100, 'book_hotel': 100, 'direct_deposit': 100, 'shopping_list': 100, 'change_ai_name': 100, 'meal_suggestion': 100})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT_pe-E4Xa9z"
      },
      "source": [
        "Using the Buket Iterator so that it forms the batch in a way that minimum padding is required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5FqpI_SQ22z"
      },
      "source": [
        "#check whether cuda is available\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \r\n",
        "\r\n",
        "#Reproducing same results\r\n",
        "SEED = 4\r\n",
        "\r\n",
        "#Torch\r\n",
        "torch.manual_seed(SEED)\r\n",
        "\r\n",
        "#Cuda algorithms\r\n",
        "torch.backends.cudnn.deterministic = True  \r\n",
        "\r\n",
        "#set batch size\r\n",
        "BATCH_SIZE = 128\r\n",
        "\r\n",
        "#Load an iterator\r\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\r\n",
        "    (train, val, test), \r\n",
        "    batch_size = BATCH_SIZE,\r\n",
        "    sort_key = lambda x: len(x.text),\r\n",
        "    sort_within_batch = True,\r\n",
        "    device = device)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS3d8M6XXj0J"
      },
      "source": [
        "Creating the Bi-directional Deep LSTM Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ObSIgWpb09l"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "\r\n",
        "class classifier(nn.Module):\r\n",
        "    \r\n",
        "    #define all the layers used in model\r\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \r\n",
        "                 bidirectional, dropout):\r\n",
        "        \r\n",
        "        #Constructor\r\n",
        "        super().__init__()          \r\n",
        "        \r\n",
        "        #embedding layer\r\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\r\n",
        "        \r\n",
        "        #lstm layer\r\n",
        "        self.lstm = nn.LSTM(embedding_dim, \r\n",
        "                           hidden_dim, \r\n",
        "                           num_layers=n_layers, \r\n",
        "                           bidirectional=bidirectional, \r\n",
        "                           dropout=dropout,\r\n",
        "                           batch_first=True)\r\n",
        "        \r\n",
        "        #dense layer\r\n",
        "        # self.fc1 = nn.Linear(hidden_dim * 2, 64)\r\n",
        "        # self.fc2 = nn.Linear(64, output_dim)\r\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\r\n",
        "        #activation function\r\n",
        "        # self.act = nn.ReLU()\r\n",
        "        self.act = nn.Sigmoid()\r\n",
        "        \r\n",
        "    def forward(self, text, text_lengths):\r\n",
        "        \r\n",
        "        #text = [batch size,sent_length]\r\n",
        "        embedded = self.embedding(text)\r\n",
        "        #embedded = [batch size, sent_len, emb dim]\r\n",
        "      \r\n",
        "        #packed sequence\r\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\r\n",
        "        \r\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\r\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\r\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\r\n",
        "        \r\n",
        "        #concat the final forward and backward hidden state\r\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\r\n",
        "                \r\n",
        "        #hidden = [batch size, hid dim * num directions]\r\n",
        "        # dense_outputs1=self.act(self.fc1(hidden))\r\n",
        "        # dense_outputs=self.fc2(dense_outputs1)\r\n",
        "        dense_outputs=self.fc(hidden)\r\n",
        "        #Final activation function\r\n",
        "        outputs=self.act(dense_outputs)\r\n",
        "        \r\n",
        "        return outputs"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Siqwcq2lXpMJ"
      },
      "source": [
        "Model parameters and initialization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXboT0yAjhia"
      },
      "source": [
        "#define hyperparameters\r\n",
        "size_of_vocab = len(TEXT.vocab)\r\n",
        "embedding_dim = 100\r\n",
        "num_hidden_nodes = 256\r\n",
        "num_output_nodes = 20\r\n",
        "num_layers = 2\r\n",
        "bidirection = True\r\n",
        "dropout = 0.25\r\n",
        "\r\n",
        "#instantiate the model\r\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers, \r\n",
        "                   bidirectional = bidirection, dropout = dropout)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJMWl9CPXuG3"
      },
      "source": [
        "Summary of the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIz2ahFwkKfC",
        "outputId": "62857ff4-0acc-4c9d-a882-db952e907128"
      },
      "source": [
        "#architecture\r\n",
        "print(model)\r\n",
        "\r\n",
        "#No. of trianable parameters\r\n",
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "    \r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\r\n",
        "\r\n",
        "#Initialize the pretrained embedding\r\n",
        "pretrained_embeddings = TEXT.vocab.vectors\r\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\r\n",
        "\r\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(580, 100)\n",
            "  (lstm): LSTM(100, 256, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=20, bias=True)\n",
            "  (act): Sigmoid()\n",
            ")\n",
            "The model has 2,378,404 trainable parameters\n",
            "torch.Size([580, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43c4nX11X-cY"
      },
      "source": [
        "Defining optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSjk6ijJkMEI"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "\r\n",
        "#define optimizer and loss\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "    \r\n",
        "#push to cuda if available\r\n",
        "model = model.to(device)\r\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ECS0_etp7TY"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\r\n",
        "    \r\n",
        "    #initialize every epoch \r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "    \r\n",
        "    #set the model in training phase\r\n",
        "    model.train()  \r\n",
        "    \r\n",
        "    for batch in iterator:\r\n",
        "        \r\n",
        "        #resets the gradients after every batch\r\n",
        "        optimizer.zero_grad()   \r\n",
        "        \r\n",
        "        #retrieve text and no. of words\r\n",
        "        text, text_lengths = batch.text   \r\n",
        "        \r\n",
        "        #convert to 1D tensor\r\n",
        "        predictions = model(text, text_lengths).squeeze()  \r\n",
        "        \r\n",
        "        #compute the loss\r\n",
        "        # print(\"Pred Shape\", predictions)\r\n",
        "        # print(\"Label Shape\", batch.label)\r\n",
        "        # exit()\r\n",
        "        loss = criterion(predictions, batch.label.long())        \r\n",
        "        \r\n",
        "        #compute the accuracy\r\n",
        "        num_corrects = (torch.max(predictions, 1)[1].view(batch.label.size()).data == batch.label.data).sum()\r\n",
        "        acc = 100.0 * num_corrects/len(batch)   \r\n",
        "        #backpropage the loss and compute the gradients\r\n",
        "        loss.backward()       \r\n",
        "        \r\n",
        "        #update the weights\r\n",
        "        optimizer.step()      \r\n",
        "        \r\n",
        "        #loss and accuracy\r\n",
        "        epoch_loss += loss.item()  \r\n",
        "        epoch_acc += acc.item()    \r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TniMTipN4i6J"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    #initialize every epoch\r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "\r\n",
        "    #deactivating dropout layers\r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    #deactivates autograd\r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for batch in iterator:\r\n",
        "        \r\n",
        "            #retrieve text and no. of words\r\n",
        "            text, text_lengths = batch.text\r\n",
        "            \r\n",
        "            #convert to 1d tensor\r\n",
        "            predictions = model(text, text_lengths).squeeze()\r\n",
        "            \r\n",
        "            #compute loss and accuracy\r\n",
        "            loss = criterion(predictions, batch.label.long())\r\n",
        "            num_corrects = (torch.max(predictions, 1)[1].view(batch.label.size()).data == batch.label.data).sum()\r\n",
        "            # print(\"Num Corrects: \", len(batch))\r\n",
        "            acc = 100.0 * num_corrects/len(batch)  \r\n",
        "            \r\n",
        "            #keep track of loss and accuracy\r\n",
        "            epoch_loss += loss.item()\r\n",
        "            epoch_acc += acc.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0rSF71mqeJG",
        "outputId": "60c87734-6c0d-4a23-baf4-cf069b1913d2"
      },
      "source": [
        "N_EPOCHS = 40\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "     \r\n",
        "    #train the model\r\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\r\n",
        "    \r\n",
        "    #evaluate the model\r\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\r\n",
        "    \r\n",
        "    #save the best model\r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'model.pt')\r\n",
        "    \r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc:.2f}%')"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 2.994 | Train Acc: 6.23%\n",
            "\t Val. Loss: 2.992 |  Val. Acc: 7.03%\n",
            "\tTrain Loss: 2.989 | Train Acc: 21.96%\n",
            "\t Val. Loss: 2.987 |  Val. Acc: 35.94%\n",
            "\tTrain Loss: 2.983 | Train Acc: 44.25%\n",
            "\t Val. Loss: 2.980 |  Val. Acc: 44.73%\n",
            "\tTrain Loss: 2.971 | Train Acc: 47.16%\n",
            "\t Val. Loss: 2.962 |  Val. Acc: 41.41%\n",
            "\tTrain Loss: 2.941 | Train Acc: 35.77%\n",
            "\t Val. Loss: 2.885 |  Val. Acc: 36.52%\n",
            "\tTrain Loss: 2.843 | Train Acc: 36.33%\n",
            "\t Val. Loss: 2.751 |  Val. Acc: 47.07%\n",
            "\tTrain Loss: 2.701 | Train Acc: 60.15%\n",
            "\t Val. Loss: 2.613 |  Val. Acc: 64.84%\n",
            "\tTrain Loss: 2.578 | Train Acc: 69.83%\n",
            "\t Val. Loss: 2.533 |  Val. Acc: 72.07%\n",
            "\tTrain Loss: 2.495 | Train Acc: 78.40%\n",
            "\t Val. Loss: 2.476 |  Val. Acc: 67.19%\n",
            "\tTrain Loss: 2.437 | Train Acc: 82.29%\n",
            "\t Val. Loss: 2.420 |  Val. Acc: 71.48%\n",
            "\tTrain Loss: 2.394 | Train Acc: 85.85%\n",
            "\t Val. Loss: 2.398 |  Val. Acc: 79.88%\n",
            "\tTrain Loss: 2.356 | Train Acc: 88.20%\n",
            "\t Val. Loss: 2.360 |  Val. Acc: 79.49%\n",
            "\tTrain Loss: 2.326 | Train Acc: 91.16%\n",
            "\t Val. Loss: 2.337 |  Val. Acc: 84.38%\n",
            "\tTrain Loss: 2.300 | Train Acc: 91.94%\n",
            "\t Val. Loss: 2.316 |  Val. Acc: 85.94%\n",
            "\tTrain Loss: 2.281 | Train Acc: 93.30%\n",
            "\t Val. Loss: 2.298 |  Val. Acc: 84.96%\n",
            "\tTrain Loss: 2.264 | Train Acc: 94.03%\n",
            "\t Val. Loss: 2.274 |  Val. Acc: 89.45%\n",
            "\tTrain Loss: 2.245 | Train Acc: 94.97%\n",
            "\t Val. Loss: 2.268 |  Val. Acc: 87.50%\n",
            "\tTrain Loss: 2.233 | Train Acc: 95.55%\n",
            "\t Val. Loss: 2.251 |  Val. Acc: 92.19%\n",
            "\tTrain Loss: 2.219 | Train Acc: 95.56%\n",
            "\t Val. Loss: 2.241 |  Val. Acc: 88.48%\n",
            "\tTrain Loss: 2.208 | Train Acc: 96.44%\n",
            "\t Val. Loss: 2.234 |  Val. Acc: 89.06%\n",
            "\tTrain Loss: 2.197 | Train Acc: 96.63%\n",
            "\t Val. Loss: 2.224 |  Val. Acc: 90.23%\n",
            "\tTrain Loss: 2.189 | Train Acc: 96.53%\n",
            "\t Val. Loss: 2.214 |  Val. Acc: 90.23%\n",
            "\tTrain Loss: 2.182 | Train Acc: 96.68%\n",
            "\t Val. Loss: 2.211 |  Val. Acc: 91.41%\n",
            "\tTrain Loss: 2.178 | Train Acc: 96.53%\n",
            "\t Val. Loss: 2.211 |  Val. Acc: 90.04%\n",
            "\tTrain Loss: 2.169 | Train Acc: 97.41%\n",
            "\t Val. Loss: 2.199 |  Val. Acc: 90.62%\n",
            "\tTrain Loss: 2.163 | Train Acc: 97.36%\n",
            "\t Val. Loss: 2.204 |  Val. Acc: 92.58%\n",
            "\tTrain Loss: 2.158 | Train Acc: 97.36%\n",
            "\t Val. Loss: 2.189 |  Val. Acc: 93.16%\n",
            "\tTrain Loss: 2.154 | Train Acc: 97.61%\n",
            "\t Val. Loss: 2.183 |  Val. Acc: 93.36%\n",
            "\tTrain Loss: 2.150 | Train Acc: 97.56%\n",
            "\t Val. Loss: 2.184 |  Val. Acc: 92.97%\n",
            "\tTrain Loss: 2.146 | Train Acc: 97.75%\n",
            "\t Val. Loss: 2.184 |  Val. Acc: 93.16%\n",
            "\tTrain Loss: 2.143 | Train Acc: 97.61%\n",
            "\t Val. Loss: 2.181 |  Val. Acc: 91.60%\n",
            "\tTrain Loss: 2.141 | Train Acc: 97.51%\n",
            "\t Val. Loss: 2.171 |  Val. Acc: 93.75%\n",
            "\tTrain Loss: 2.137 | Train Acc: 97.80%\n",
            "\t Val. Loss: 2.178 |  Val. Acc: 91.99%\n",
            "\tTrain Loss: 2.135 | Train Acc: 97.75%\n",
            "\t Val. Loss: 2.183 |  Val. Acc: 92.38%\n",
            "\tTrain Loss: 2.133 | Train Acc: 97.95%\n",
            "\t Val. Loss: 2.196 |  Val. Acc: 90.82%\n",
            "\tTrain Loss: 2.130 | Train Acc: 97.85%\n",
            "\t Val. Loss: 2.187 |  Val. Acc: 91.80%\n",
            "\tTrain Loss: 2.128 | Train Acc: 97.85%\n",
            "\t Val. Loss: 2.180 |  Val. Acc: 92.38%\n",
            "\tTrain Loss: 2.126 | Train Acc: 98.24%\n",
            "\t Val. Loss: 2.188 |  Val. Acc: 92.19%\n",
            "\tTrain Loss: 2.125 | Train Acc: 97.90%\n",
            "\t Val. Loss: 2.189 |  Val. Acc: 91.21%\n",
            "\tTrain Loss: 2.122 | Train Acc: 98.05%\n",
            "\t Val. Loss: 2.197 |  Val. Acc: 90.82%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5ekmZDh-GcN",
        "outputId": "252cfa18-fca5-431c-c556-d156ea7d3496"
      },
      "source": [
        "#load weights\r\n",
        "path='/content/model.pt'\r\n",
        "model.load_state_dict(torch.load(path));\r\n",
        "model.eval();\r\n",
        "test_loss, test_acc = evaluate(model, valid_iterator, criterion)\r\n",
        "print(f'\\tTest Loss: {test_loss:.3f} | Test Acc: {test_acc:.2f}%')"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTest Loss: 2.171 | Test Acc: 93.75%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}